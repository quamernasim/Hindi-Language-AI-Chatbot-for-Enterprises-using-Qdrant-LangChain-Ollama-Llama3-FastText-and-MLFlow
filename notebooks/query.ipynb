{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_lgging = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mlflow_lgging:\n",
    "    mlflow.set_experiment(\"Hindi Chatbot\")\n",
    "    mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 6333\n",
    "client = QdrantClient(host=host, port=port)\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"qdrant_host\", host)\n",
    "    mlflow.log_param(\"qdrant_port\", port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext as ft\n",
    "\n",
    "embed_model_path = 'wiki.hi.bin'\n",
    "embed_model = ft.load_model(embed_model_path)\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"embed_model_path\", embed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "import fasttext as ft\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "class QdrantRetriever(BaseRetriever):\n",
    "    client: QdrantClient\n",
    "    embed_model: ft.FastText._FastText\n",
    "    collection_name: str\n",
    "    limit: int\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "        \"\"\"Converts query to a vector and retrieves relevant documents using Qdrant.\"\"\"\n",
    "        # client = QdrantClient(host=\"localhost\", port=6333)\n",
    "        query_vector = self.embed_model.get_sentence_vector(query).tolist()\n",
    "        search_results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=self.limit\n",
    "        )\n",
    "        return [Document(page_content=hit.payload['page_content']) for hit in search_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = 'my_collection'\n",
    "limit = 1\n",
    "\n",
    "retriever = QdrantRetriever(\n",
    "    client=client,\n",
    "    embed_model=embed_model,\n",
    "    collection_name=collection_name,\n",
    "    limit=limit\n",
    ")\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"collection_name\", collection_name)\n",
    "    mlflow.log_param(\"limit\", limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "model_name = 'llama3'\n",
    "num_predict = 100\n",
    "num_ctx = 3000\n",
    "num_gpu = 2\n",
    "temperature = 0.7\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "llm=Ollama(model='llama3', num_predict=100, num_ctx=3000, num_gpu=2, temperature=0.7, top_k=50, top_p=0.95)\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"num_predict\", num_predict)\n",
    "    mlflow.log_param(\"num_ctx\", num_ctx)\n",
    "    mlflow.log_param(\"num_gpu\", num_gpu)\n",
    "    mlflow.log_param(\"temperature\", temperature)\n",
    "    mlflow.log_param(\"top_k\", top_k)\n",
    "    mlflow.log_param(\"top_p\", top_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है। आप केवल हिंदी भाषा में उत्तर दे सकते हैं। धन्यवाद।\n",
    "    \n",
    "    You are never ever going to generate response in English. You are always going to generate response in Hindi no matter what. You also need to keep your answer short and to the point.\n",
    "\n",
    "    संदर्भ: {context} </s>\n",
    "\"\"\"\n",
    ") \n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"system_prompt\", system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?'\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"query\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"context\", response['context'])\n",
    "    mlflow.log_param(\"response\", response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def answer_question(query, history):\n",
    "    response = chain.invoke({\"input\": query})\n",
    "    return response['answer']\n",
    "\n",
    "gr.ChatInterface(answer_question).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Approach - Simple and Straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "\n",
    "# client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# import fasttext as ft\n",
    "# # Loding model for Hindi.\n",
    "# embed_model = ft.load_model('wiki.hi.bin')\n",
    "\n",
    "# query = 'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?'\n",
    "\n",
    "# hits = client.search(\n",
    "# collection_name=\"my_collection\",\n",
    "# query_vector= embed_model.get_sentence_vector(query).tolist(),\n",
    "# limit=1,\n",
    "# )\n",
    "\n",
    "\n",
    "# context = ''\n",
    "# for hit in hits:\n",
    "#     context += hit.payload['page_content'] + '\\n'\n",
    "\n",
    "\n",
    "# prompt = f\"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है। आप केवल हिंदी भाषा में उत्तर दे सकते हैं। धन्यवाद।\n",
    "#     संदर्भ: {context}\n",
    "#     प्रश्न: {query} [/INST] </s>\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# from langchain_community.llms.ollama import Ollama\n",
    "# llm=Ollama(model='llama3', num_predict=100, num_ctx=3000, num_gpu=2, temperature=0.7, top_k=50, top_p=0.95)\n",
    "\n",
    "# llm.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
