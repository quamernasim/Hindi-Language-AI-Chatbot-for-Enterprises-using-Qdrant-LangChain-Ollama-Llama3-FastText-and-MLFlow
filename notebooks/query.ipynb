{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start building the next part of the chatbot. In this part, we will be using the LLM from the Ollama and integrating it with the chatbot. More particularly we will be using the Llama3 model. Llama 3 is Meta's latest and most advanced open-source large language model (LLM). It is the successor to the previous Llama 2 model and represents a significant improvement in performance across a variety of benchmarks and tasks. Llama 3 comes in two main versions - an 8 billion parameter model and a 70 billion parameter model. Llama 3 supports longer context lengths of up to 8,000 tokens\n",
    "\n",
    "We will be using the MLFlow to track all the configurations and the results of the model. let's first insall the Ollama, get the llama3 model from the ollama and the MLFlow.\n",
    "\n",
    "```\n",
    "# install the Ollama\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# get the llama3 model\n",
    "ollama pull llama2\n",
    "\n",
    "# install the MLFlow\n",
    "pip install mlflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed_model_path = '../wiki.hi.bin'\n",
    "# embed_model_path = '../indicnlp.ft.hi.300.bin'\n",
    "\n",
    "collection_name = 'my_collection'\n",
    "limit = 1\n",
    "\n",
    "model_name = 'llama3'\n",
    "num_predict = 100\n",
    "num_ctx = 3000\n",
    "num_gpu = 2\n",
    "temperature = 0.7\n",
    "top_k = 50\n",
    "top_p = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start by loading the qdrant client that will be used to retrieve the context for a given query. We will also start logging the configurations and the results of the workflows using the MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/30 18:48:45 INFO mlflow.tracking.fluent: Experiment with name 'Hindi Chatbot' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "mlflow_lgging = True\n",
    "\n",
    "if mlflow_lgging:\n",
    "    # set the experiment name in the mlflow\n",
    "    mlflow.set_experiment(\"Hindi Chatbot\")\n",
    "    # start the mlflow run\n",
    "    mlflow.start_run()\n",
    "\n",
    "# load the Qdrant client from the same host and port\n",
    "# this client will be used to interact with the Qdrant server\n",
    "host = \"localhost\"\n",
    "port = 6333\n",
    "client = QdrantClient(host=host, port=port)\n",
    "\n",
    "# log the parameters in the mlflow\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"qdrant_host\", host)\n",
    "    mlflow.log_param(\"qdrant_port\", port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load the embedding model. This embedding model is necessary to convert the query to the embedding that can be used to do a similarity search in the qdrant. The ultimate goal is to retrieve the context for a given query based on the similarity of the query embedding with the context embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext as ft\n",
    "\n",
    "# You will need to download these models from the URL mentioned below\n",
    "embedding_model_path = '../wiki.hi.bin' #https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.hi.zip\n",
    "# embedding_model_path = '../indicnlp.ft.hi.300.bin' #https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/embedding-v2/indicnlp.ft.hi.300.bin\n",
    "embed_model = ft.load_model(embed_model_path)\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"embed_model_path\", embed_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain by default does not support the FastText embedding framework.It only supports Huggingface and OpenAI models. So that is why we need to define the custom langchain retriever class that will be used to retrieve the context for a given query. In this class, we will have one method _get_relevant_documents which will do the similarity seach in the qdrant based on the FastText embedding model and return the context for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "import fasttext as ft\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "# Define a custom retriever class that uses Qdrant for document retrieval\n",
    "# Since we're using FastText embeddings, we won't be able to use the default lanchain retriever, as it only supports HuggingFace and OpenAI Models\n",
    "class QdrantRetriever(BaseRetriever):\n",
    "    client: QdrantClient\n",
    "    embed_model: ft.FastText._FastText\n",
    "    collection_name: str\n",
    "    limit: int\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "        \"\"\"Converts query to a vector and retrieves relevant documents using Qdrant.\"\"\"\n",
    "        # Get the vector representation of the query using the FastText model\n",
    "        query_vector = self.embed_model.get_sentence_vector(query).tolist()\n",
    "\n",
    "        # Search for the most similar documents in the Qdrant collection\n",
    "        # The search method returns a list of hits, where each hit contains the most similar document\n",
    "        # we can limit the number of hits to return using the limit parameter\n",
    "        search_results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=self.limit\n",
    "        )\n",
    "        # Finally, we convert the search results to a list of Document objects\n",
    "        # that can be used by the pipeline\n",
    "        return [Document(page_content=hit.payload['page_content']) for hit in search_results]\n",
    "\n",
    "# use the QdrantRetriever class to create a retriever object\n",
    "retriever = QdrantRetriever(\n",
    "    client=client,\n",
    "    embed_model=embed_model,\n",
    "    collection_name=collection_name,\n",
    "    limit=limit\n",
    ")\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"collection_name\", collection_name)\n",
    "    mlflow.log_param(\"limit\", limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load the Llama3 model. We will be using the 8 billion parameter model. Instead of using huggingface to load the model, we will be using the Ollama to load the model. The Ollama provides a simple and easy way to load the models without much of a hassle. The class Ollama takes in a number of arguments out of which the most important ones are num_predict (number of tokens to be generated), num_ctx (maximum context size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "# Create an Ollama object with the specified parameters\n",
    "# This will very easily load the llama3 8-B model without the need of separately handling tokenizer like we do in huggingface\n",
    "llm=Ollama(model='llama3', num_predict=100, num_ctx=3000, num_gpu=2, temperature=0.7, top_k=50, top_p=0.95)\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"num_predict\", num_predict)\n",
    "    mlflow.log_param(\"num_ctx\", num_ctx)\n",
    "    mlflow.log_param(\"num_gpu\", num_gpu)\n",
    "    mlflow.log_param(\"temperature\", temperature)\n",
    "    mlflow.log_param(\"top_k\", top_k)\n",
    "    mlflow.log_param(\"top_p\", top_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. So far we have been able to setup the retriver which will retrive the context from thr database based on the similarity of the query embedding with the context embeddings. We have also loaded the Llama3 model. Now there's just one more thing left to do. We need create a chat template. Chat template includes two types of prompts. First one is system prompts and another one is user prompts. System prompts are the prompts that are written to control the behavior of the chatbot or LLMs. It is very important to have a good system prompts to get responses as per expectations. A bad system prompt can lead to poor or incorrect behavior of your chatbot. I spent sometime in optimizing the system prompts to get the best results. User prompts are the prompts that are written to get the responses from the chatbot. These prompts are the questions or queries that the user wants to ask the chatbot. Just like a good system prompt, it is always recommended to have a good user prompt. It should be concise, informative and to the point. So next we create these chat templates based on these two prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है। आप केवल हिंदी भाषा में उत्तर दे सकते हैं। धन्यवाद।\n",
    "    \n",
    "    You are never ever going to generate response in English. You are always going to generate response in Hindi no matter what. You also need to keep your answer short and to the point.\n",
    "\n",
    "    संदर्भ: {context} </s>\n",
    "\"\"\"\n",
    ") \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"system_prompt\", system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tie up everything and create a chain of actions. We first want to retrive the relevant documents based on the prompt. We then want to generate the response based on the context and the prompt. create_stuff_documents_chain and create_retrieval_chain is exactly what we need to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Create a chain that combines the retriever and the question-answer chain\n",
    "# essentially, this chain will retrieve relevant documents using the retriever\n",
    "# and the prompts\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally. We have now successfully built the chatbot using the Llama3 model. Let's now test the chatbot and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?', 'context': [Document(page_content='अक्सर रचनाकारों और फिल्म निर्माताओं को ऐसी कहानियाँ आकर्षित करती रही हैं  जिनके जांबाज नायक नामी हैं और जीवित हैं  शहीदों से लेकर डाकुओं तक के जीवन ने कई फार्मूला फिल्म निर्देशकों से लेकर कला निर्देशकों तक को प्रेरित किया है  जब मैंने सुना कि राजस्थान के छोटे से गाँव भटेरी में महिला विकास कार्यक्रम में काम करने वाली  साथिन  भँवरी देवी के जीवन पर फिल्म का निर्माण हो रहा है  तो मेरे लिए यह आश्चर्य का विषय नहीं था')], 'answer': 'सामान्य तौर पर, रचनाकारों और फिल्म निर्माताओं को ऐसे किरदार और कहानी तत्व आकर्षित करते हैं जिनके साथ सम्बंधित लोग हों, या जिनके साथ उनका अपना अनुभव हो। इसके अलावा, रचनाकारों और फिल्म निर्माताओं को ऐसे किरदार'}\n"
     ]
    }
   ],
   "source": [
    "query = 'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?'\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"query\", query)\n",
    "\n",
    "response = chain.invoke({\"input\": query})\n",
    "\n",
    "if mlflow_lgging:\n",
    "    mlflow.log_param(\"context\", response['context'])\n",
    "    mlflow.log_param(\"response\", response['answer'])\n",
    "\n",
    "print(response)\n",
    "\n",
    "# end the logging of the mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'input': 'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?',\n",
    " 'context': [Document(page_content='अक्सर रचनाकारों और फिल्म निर्माताओं को ऐसी कहानियाँ आकर्षित करती रही हैं  जिनके जांबाज नायक नामी हैं और जीवित हैं  शहीदों से लेकर डाकुओं तक के जीवन ने कई फार्मूला फिल्म निर्देशकों से लेकर कला निर्देशकों तक को प्रेरित किया है  जब मैंने सुना कि राजस्थान के छोटे से गाँव भटेरी में महिला विकास कार्यक्रम में काम करने वाली  साथिन  भँवरी देवी के जीवन पर फिल्म का निर्माण हो रहा है  तो मेरे लिए यह आश्चर्य का विषय नहीं था')],\n",
    " 'answer': 'सामान्य तौर पर, रचनाकारों और फिल्म निर्माताओं को ऐसे किरदार और कहानी तत्व आकर्षित करते हैं जिनके साथ सम्बंधित लोग हों, या जिनके साथ उनका अपना अनुभव हो। इसके अलावा, रचनाकारों और फिल्म निर्माताओं को ऐसे किरदार'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have been logging the parameters of the chatbot in the MLFlow. Let's now check that out and see how it looks.\n",
    "\n",
    "```\n",
    "# launches the MLFlow dashboard\n",
    "mlflow ui --port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. In this blog we saw how we can use LangChain, Ollama, Qdrant, MLFlow, and Llama3 Model to build a hindi language chatbot. We also saw how we can track the parameters and the results of the chatbot using the MLFlow. As a bonus, let's also build a gradio UI for the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def answer_question(query, history):\n",
    "    response = chain.invoke({\"input\": query})\n",
    "    return response['answer']\n",
    "\n",
    "gr.ChatInterface(answer_question).launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this blog. I hope you enjoyed this blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Approach - Simple and Straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "\n",
    "# client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# import fasttext as ft\n",
    "# # Loding model for Hindi.\n",
    "# embed_model = ft.load_model('wiki.hi.bin')\n",
    "\n",
    "# query = 'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?'\n",
    "\n",
    "# hits = client.search(\n",
    "# collection_name=\"my_collection\",\n",
    "# query_vector= embed_model.get_sentence_vector(query).tolist(),\n",
    "# limit=1,\n",
    "# )\n",
    "\n",
    "\n",
    "# context = ''\n",
    "# for hit in hits:\n",
    "#     context += hit.payload['page_content'] + '\\n'\n",
    "\n",
    "\n",
    "# prompt = f\"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है। आप केवल हिंदी भाषा में उत्तर दे सकते हैं। धन्यवाद।\n",
    "#     संदर्भ: {context}\n",
    "#     प्रश्न: {query} [/INST] </s>\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# from langchain_community.llms.ollama import Ollama\n",
    "# llm=Ollama(model='llama3', num_predict=100, num_ctx=3000, num_gpu=2, temperature=0.7, top_k=50, top_p=0.95)\n",
    "\n",
    "# llm.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
