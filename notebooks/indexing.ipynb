{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's digital era, where businesses are increasingly leveraging technology to enhance customer interactions, AI-powered chatbots have emerged as a game-changer. These chatbots can have a natural conversation with users, providing real-time support and information. Though chatbots have become popular since last two years, most of them are designed to interact in English. However, in a country like India, where Hindi is spoken by millions as a first language, there is a need for chatbots that can interact in Hindi. Building a Hindi-language chatbot can help businesses cater to a wider audience and provide better customer service. In this blog, we will discuss the technical journey of building a Hindi-language AI chatbot for enterprises. By the end of this blog, you will understand the challenges associated with building a Hindi-language chatbot and how to overcome them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an AI chatbot is a two step process: Indexing and Querying. In the indexing phase, we will create a database of Hindi-language documents for the chatbot to refer to. This data is basically going to be the knowledge base of the chatbot. It can be a collection of FAQs, product manuals, or any other information that the chatbot needs to refer to while interacting with users. In the querying phase, we will use this indexed data to answer user queries with the help of an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog, I will be using the following tools and frameworks for building the RAG based AI-Powered Hindi Chatbot:\n",
    "\n",
    "- LangChain: I'll be using LangChain to build the RAG application, which will enhance the chatbot's ability to generate responses by leveraging information retrieved from a knowledge base.\n",
    "- Qdrant: I'll be using Qdrant as the vector database to store the documents and their corresponding embeddings.\n",
    "- FastText: I'll be using FastText as the language embedding framework for loading the Hindi language embeddings model.\n",
    "- Ollama: Ollama will help us load the LLM very easily. We'll integrate the Ollama with LangChain to load the LLM.\n",
    "- MLFlow: I'll be using MLFlow to manage the configurations of the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Hindi-Aesthetics-Corpus/Corpus'\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "batch_size = 4000\n",
    "host = 'localhost'\n",
    "port = 6333\n",
    "embedding_model_path = '../wiki.hi.bin' #https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.hi.zip\n",
    "# embedding_model_path = '../indicnlp.ft.hi.300.bin' #https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/embedding-v2/indicnlp.ft.hi.300.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating a knowledge base of our chatbot i'll be using the Hindi Aesthetic Corpus dataset. This dataset contains a large number of Hindi texts, more than 1000 text files. You can replace this dataset with your own business related data. It can be a collection of FAQs, product manuals, or any other information that you want your chatbot to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the process of indexing the data, we first need to load the dataset. As mentioned earlier, we will be using the Hindi Aesthetic Corpus dataset. Once the dataset is loaded, we will split the text into chunks using the RecursiveCharacterTextSplitter. Creating smaller chunks of text is essential since LLMs comes with a limited context size. Having smaller and relevant context will help us in two ways: First we will only have high quality and relevant context for the LLM to learn from. Second, processing larger chunk or context means more tokens that needs to be processed, which will increase the total runtime and be financially expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the documents from the directory\n",
    "loader = DirectoryLoader(data_path, loader_cls=TextLoader)\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have converted the raw data into smaller chunks of text, we will then convert these chunks into embeddings using the FastText model. In this blog, we experimented with two different embeddings models: Hindi Model by FastText and IndicFT. The performance of IndicFT was not that good, so we decided to go with the FastText model. We will use the FastText model to convert the text into embeddings. These embeddings will be stored in a vector database using Qdrant. The embeddings will be used to retrieve the most relevant documents for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext as ft\n",
    "\n",
    "# You will need to download these models from the URL mentioned below\n",
    "embedding_model_path = '../wiki.hi.bin' #https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.hi.zip\n",
    "# embedding_model_path = '../indicnlp.ft.hi.300.bin' #https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/embedding-v2/indicnlp.ft.hi.300.bin\n",
    "embed_model = ft.load_model(embedding_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have dowloaded the hindi embedding model, let's proceed to generate the embeddings for each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert the documents to a dataframe\n",
    "# This dataframe will be used to create the embeddings\n",
    "# And later will be used to update the Qdrant Vector Database\n",
    "data = []\n",
    "for doc in docs:\n",
    "    # Get the page content and metadata for each chunk\n",
    "    # Meta data contains chunk source or file name\n",
    "    row_data = {\n",
    "        \"page_content\": doc.page_content,\n",
    "        \"metadata\": doc.metadata\n",
    "    }\n",
    "    data.append(row_data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Replace the new line characters with space\n",
    "df['page_content'] = df['page_content'].replace('\\\\n', ' ', regex=True)\n",
    "\n",
    "# Create a unique id for each document.\n",
    "# This id will be used to update the Qdrant Vector Database\n",
    "df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Create a payload column in the dataframe\n",
    "# This payload column includes the page content and metadata\n",
    "# This payload will be used when LLM needs to answer a query\n",
    "df['payload'] = df[['page_content', 'metadata']].to_dict(orient='records')\n",
    "\n",
    "# Create embeddings for each chunk\n",
    "# This embeddings will be used when doing a similarity search with the user query\n",
    "df['embeddings'] = df['page_content'].apply(lambda x: (embed_model.get_sentence_vector(x)).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now that we have the embeddings, we need to store them in a vector database. We will be using Qdrant for this purpose. Qdrant is an open-source vector database that allows you to store and query high-dimensional vectors. The easiest way to get started with the Qdrant database is using the docker. Follow the below steps to get the Qdrant database up and running:\n",
    "\n",
    "```\n",
    "# Run the following command in terminal to get the docker image of the qdrant\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "\n",
    "# Run the following command in terminal to start the qdrant server\n",
    "docker run -p 6333:6333 -v Hindi-Language-AI-Chatbot-for-Enterprises-using-Qdrant-MLFlow-and-LangChain/:/qdrant/storage qdrant/qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open a connection to the Qdrant database using the qdrant_client. We then need to create a new collection in the Qdrant database in which we will store the embeddings. Once this is done, we will insert the embeddings, along with the corresponding document IDs and payloads, into the collection. The document IDs will be used to identify the documents, the payloads will contain the actual text of the document and the embeddings will be used to retrieve the most relevant documents for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, Batch\n",
    "\n",
    "# Create a QdrantClient object\n",
    "host = 'localhost'\n",
    "port = 6333\n",
    "client = QdrantClient(host=host, port=port)\n",
    "\n",
    "# delete the collection if it already exists\n",
    "client.delete_collection(collection_name=\"my_collection\")\n",
    "\n",
    "# Create a fresh collection in Qdrant\n",
    "client.recreate_collection(\n",
    "   collection_name=\"my_collection\",\n",
    "   vectors_config=VectorParams(size=300, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Update the Qdrant Vector Database with the embeddings\n",
    "# We are updating the embeddings in batches\n",
    "# Since the data is large, we will only update the first batch of size 4000\n",
    "batch_size = 4000\n",
    "client.upsert(\n",
    " collection_name=\"my_collection\",\n",
    " points=Batch(\n",
    "     ids=df['id'].to_list()[:batch_size],\n",
    "     payloads=df['payload'][:batch_size],\n",
    "     vectors=df['embeddings'].to_list()[:batch_size],\n",
    " ),\n",
    ")\n",
    "\n",
    "# Close the QdrantClient\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the embeddings in the Qdrant database, we can view the collection in the Qdrant dashboard. We can see from the dashboard that each chunk has got 3 infortmation: metadata, chunk text and embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
