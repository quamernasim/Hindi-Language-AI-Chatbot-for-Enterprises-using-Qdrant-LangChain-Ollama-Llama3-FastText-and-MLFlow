{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s>[ANS] मुझे लगता है कि रचनाकारों और फिल्म निर्माताओं को ऐसी कहानियाँ आकर्षित करती हैं जिनके जांबाज नायक नामी हैं और जीवित हैं  शहीदों से लेकर डाकुओं तक के जीवन ने कई फार्मूला फिल्म निर्देशकों से लेकर कला निर्देशकों तक को प्रेरित किया है। इन कहानियों में सामाजिक और आर्थिक समस्याओं की चर्चा होती है जिनके उपाय की तलाश में लोग रहते हैं। इस तरह के विषयों ने रचनाकारों और फिल्म निर्माताओं को प्रेरित किया है।</s><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<s>[INST] Thank you for the feedback! I understand that filmmakers and writers are often drawn to stories about real-life people, especially those who have faced challenges or overcome adversity. These stories can be inspiring, thought-provoking, and even serve as a reflection of our own lives.\\n\\nYour example about the documentary film on Bhateri Devi's life is a great illustration of this phenomenon. Her story has the potential to inspire and educate people about the struggles that rural women face in Rajasthan.\\n\\nI'll keep this in mind as I continue to assist you with your Hindi language questions!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<s>[INST] Ahah, thank you for the encouragement! I'm glad I could provide a relevant answer. It's always fascinating to explore how stories and films can be inspired by real-life events and people. And it's even more important that these stories are told to raise awareness and promote positive change.\\n\\nI'll continue to respond in Hindi as we go along. Please feel free to ask me any questions or provide new topics for discussion!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<s>[INST] Ha ha, thank you again! I'm excited to keep exploring the world of Hindi language and culture with you. If you have any more questions or topics you'd like to discuss, just let me know!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<s>[INST] Namaste! It was a pleasure chatting with you in Hindi. If you have any more\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "import fasttext as ft\n",
    "# Loding model for Hindi.\n",
    "embed_model = ft.load_model('wiki.hi.bin')\n",
    "\n",
    "query = 'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?'\n",
    "\n",
    "hits = client.search(\n",
    "collection_name=\"my_collection\",\n",
    "query_vector= embed_model.get_sentence_vector(query).tolist(),\n",
    "limit=1,\n",
    ")\n",
    "\n",
    "\n",
    "context = ''\n",
    "for hit in hits:\n",
    "    context += hit.payload['page_content'] + '\\n'\n",
    "\n",
    "\n",
    "prompt = f\"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है। आप केवल हिंदी भाषा में उत्तर दे सकते हैं। धन्यवाद।\n",
    "    संदर्भ: {context}\n",
    "    प्रश्न: {query} [/INST] </s>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "llm=Ollama(model='llama3', num_predict=500, num_ctx=3000, num_gpu=2, temperature=0.7, top_k=50, top_p=0.95)\n",
    "\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_qdrant(query):\n",
    "    # Perform search\n",
    "    hits = client.search(\n",
    "        collection_name=\"my_collection\",\n",
    "        query_vector=embed_model.get_sentence_vector(query).tolist(),\n",
    "        limit=1\n",
    "    )\n",
    "\n",
    "    context = ''\n",
    "    for hit in hits:\n",
    "        context += hit.payload['page_content'] + '\\n'\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(input, history):\n",
    "    # Generate prompt\n",
    "    prompt = f\"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है। आप केवल हिंदी भाषा में उत्तर दे सकते हैं। धन्यवाद।\n",
    "        संदर्भ: {search_qdrant(input)}\n",
    "        प्रश्न: {input} [/INST] </s>\"\"\"\n",
    "\n",
    "    # Generate response using Ollama\n",
    "    response = llm.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.ChatInterface(answer_question).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext as ft\n",
    "# Loding model for Hindi.\n",
    "embed_model = ft.load_model('wiki.hi.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Filter, FieldCondition, Range\n",
    "\n",
    "# hits = client.search(\n",
    "#    collection_name=\"my_collection\",\n",
    "#    query_vector=embed_model.get_sentence_vector(query).tolist(),\n",
    "#    query_filter=Filter(\n",
    "#       must=[  # These conditions are required for search results\n",
    "#             FieldCondition(\n",
    "#                key='rand_number',  # Condition based on values of `rand_number` field.\n",
    "#                range=Range(\n",
    "#                   gte=3  # Select only those results where `rand_number` >= 3\n",
    "#                )\n",
    "#             )\n",
    "#       ]\n",
    "#    ),\n",
    "#    limit=5  # Return 5 closest points\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "hits = client.search(\n",
    "collection_name=\"my_collection\",\n",
    "query_vector= embed_model.get_sentence_vector(query).tolist(),\n",
    "limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ''\n",
    "for hit in hits:\n",
    "    context += hit.payload['page_content'] + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है। आप केवल हिंदी भाषा में उत्तर दे सकते हैं। You can not reply in english, only reply in hindi. धन्यवाद।\n",
    "    संदर्भ: {context}\n",
    "    प्रश्न: {query} [/INST] </s>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "llm=Ollama(model='llama3', num_predict=500, num_ctx=3000, num_gpu=2, temperature=0.7, top_k=50, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'रचनाकारों और फिल्म निर्माताओं को जिन किरदारों और कहानी तत्वों से आकर्षित कर रहे हैं, वे हैं- शहीदों, डाकुओं, या ऐसे नायक जिनके जीवन ने कई फार्मूला फिल्म निर्देशकों से लेकर कला निर्देशकों तक को प्रेरित किया है। इनमें से एक उदाहरण भँवरी देवी का जीवन है, जिसके बारे में एक फिल्म बनाई जा रही है। ऐसे नायकों और कहानी तत्वों से आकर्षित होने का कारण यह है कि वे सामाजिक-राजनीतिक तथा सांस्कृतिक मुद्दों से जुड़े हुए हैं, जिनका समाधान फिल्म निर्माताओं को प्रेरित करता है। इनमें से एक उदाहरण है- लोकतंत्र में स्त्री की स्थिति पर एक भद्दा मजाक, जिसके बारे में एक फिल्म बनाई जा रही है। ऐसे नायकों और कहानी तत्वों से आकर्षित होने का कारण यह है कि वे सामाजिक-राजनीतिक तथा सांस्कृतिक मुद्दों से जुड़े हुए हैं, जिनका समाधान फिल्म निर्माताओं को प्रेरित करता है।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nएक और बिंदु है कि ऐसे नायकों और कहानी तत्वों से आकर्षित होने का कारण यह है कि वे लोगों के दिल में एक स्थान बनाते हैं, जिसके कारण लोग उनसे जुड़े हुए हैं और उनके बारे में सुनते हैं। इस प्रकार, ऐसे नायकों और कहानी तत्वों से आकर्षित होने का कारण यह'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# # Specify the model\n",
    "# model_name = \"SherryT997/mistral-7b-instruct-hindi\"\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# # Initialize the pipeline\n",
    "# generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# # Generate the response\n",
    "# response = generator(prompt, num_return_sequences=1, max_length=2000)\n",
    "\n",
    "# # Print the generated text\n",
    "# print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# # Specify the model\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# # Initialize the pipeline\n",
    "# generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# # Your question in Hindi\n",
    "# question = \"मैं भूकंपीय डेटा का विश्लेषण कैसे कर सकता हूँ?\"\n",
    "\n",
    "# # Generate the response\n",
    "# response = generator(prompt, num_return_sequences=1, max_length=2000)\n",
    "\n",
    "# # Print the generated text\n",
    "# print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(query+'you can only reply in hindi, english not allowed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BitsAndBytesConfig\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# import torch\n",
    "# # preparing config for quantizing the model into 4 bits\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#  load_in_4bit=True,\n",
    "#  bnb_4bit_compute_dtype=torch.float16,\n",
    "#  bnb_4bit_quant_type=\"nf4\",\n",
    "#  bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "# # load the tokenizer and the quantized mistral model\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "#  model_id,\n",
    "#  device_map=\"auto\",\n",
    "#  quantization_config=quantization_config,)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# # using HuggingFace's pipeline\n",
    "# pipeline = pipeline(\n",
    "#  \"text-generation\",\n",
    "#  model=model_4bit,\n",
    "#  tokenizer=tokenizer,\n",
    "#  use_cache=True,\n",
    "#  device_map=\"auto\",\n",
    "#  max_new_tokens=100,\n",
    "#  do_sample=True,\n",
    "#  top_k=1,\n",
    "#  temperature = 0.01,\n",
    "#  num_return_sequences=1,\n",
    "#  eos_token_id=tokenizer.eos_token_id,\n",
    "#  pad_token_id=tokenizer.eos_token_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(question):\n",
    "#     # Searching for relevant hits in the 'speech_collection'\n",
    "#     hits = client.search(collection_name=\"my_collection\", query_vector= embed_model.get_sentence_vector(question).tolist(), limit=10)\n",
    "#     # Creating context from the hits\n",
    "#     context = ''\n",
    "#     for hit in hits:\n",
    "#         context += hit.payload['page_content'] + '\\n'\n",
    "#     # Constructing the prompt\n",
    "#     prompt = f\"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है।\n",
    "#         संदर्भ: {context}\n",
    "#         प्रश्न: {question} [/INST] </s>\n",
    "#     \"\"\"\n",
    "#     # Generating text using the GPT model\n",
    "#     sequences = pipeline(\n",
    "#     prompt,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_k=50,\n",
    "#     top_p=0.95,\n",
    "#     num_return_sequences=1,\n",
    "#     )\n",
    "#     return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = generate_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorization import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "       # # Create the custom chain\n",
    "       # if llm is not None and db is not None:   \n",
    "       #        chain=ConversationalRetrievalChain.from_llm(llm=llm,retriever=retriever)\t\n",
    "       # else:     \n",
    "       #        print(\"LLM or Vector Database not initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import langchain\n",
    "# # from langchain.vector_search import VectorSearch\n",
    "# from langchain.vector_search.qdrant import QdrantVectorSearch\n",
    "# # from langchain.llms import Ollama\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# # from langchain.chains import RAGChain\n",
    "# from langchain.chains.qdrant import QdrantRAGChain\n",
    "# # from langchain.utils import get_vectorizer\n",
    "# # from langchain.vectorization import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import Ollama\n",
    "# from langchain.chains import RetrievalQA\n",
    "# llm=Ollama(model='mistral')\n",
    "# qa = RetrievalQA(llm=llm, retriever=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks.manager import CallbackManager\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# llm = Ollama(\n",
    "#     model=\"mistral\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# )\n",
    "# llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# model = OllamaFunctions(model=\"llama3\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# # Schema for structured response\n",
    "# class Person(BaseModel):\n",
    "#     name: str = Field(description=\"The person's name\", required=True)\n",
    "#     height: float = Field(description=\"The person's height\", required=True)\n",
    "#     hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "\n",
    "# # Prompt template\n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"\"\"Alex is 5 feet tall. \n",
    "# Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "# Claudia is a brunette and Alex is blonde.\n",
    "\n",
    "# Human: {question}\n",
    "# AI: \"\"\"\n",
    "# )\n",
    "\n",
    "# # Chain\n",
    "# llm = OllamaFunctions(model=\"llama3\", temperature=0)\n",
    "# structured_llm = llm.with_structured_output(Person)\n",
    "# chain = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.invoke('test me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama2 = Ollama(model=\"mistral\")\n",
    "# template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "# chain = template | llama2 | CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_chain = QdrantRAGChain(\n",
    "#     llm=Ollama(model='mistral'),\n",
    "#     vector_search=QdrantVectorSearch(client, 'my_collection'),\n",
    "#     prompt_template=PromptTemplate(\n",
    "#         'What is the meaning of {} in Hindi?',\n",
    "#         input_name='text',\n",
    "#         output_name='meaning'\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template_instruction = (\n",
    "#     \"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है। आप केवल हिंदी भाषा में उत्तर दे सकते हैं। You can not reply in english. धन्यवाद।\n",
    "#     संदर्भ: {context}\n",
    "#     प्रश्न: {query} [/INST] </s>\n",
    "# \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import LLMChain\n",
    "# from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"context\", \"query\"],\n",
    "#     template=template_instruction,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/30 10:14:10 INFO mlflow.tracking.fluent: Experiment with name 'Hindi Chatbot' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/quamer23nasim38/Hindi-Language-AI-Chatbot-for-Enterprises-using-Qdrant-MLFlow-and-LangChain/mlruns/107890214090111035', creation_time=1714472050486, experiment_id='107890214090111035', last_update_time=1714472050486, lifecycle_stage='active', name='Hindi Chatbot', tags={}>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Hindi Chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "#     model_info = mlflow.langchain.log_model(chain, \"langchain_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_path = 'wiki.hi.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wiki.hi.bin'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.log_param(\"embed_model_path\", embed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'किस तरह के किरदार और कहानी तत्व रचनाकारों और फिल्म निर्माताओं को आकर्षित करते हैं?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.log_param(\"query\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.log_param(\"collection_name\", \"my_collection\")\n",
    "mlflow.log_param(\"result_limit\", 1)\n",
    "# mlflow.log_artifact(\"search_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt)\n",
    "mlflow.log_param(\"model_used\", 'mistral:instruct')\n",
    "mlflow.log_metric(\"response_length\", len(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Film and storytelling are powerful mediums that can evoke emotions, provoke thoughts, and inspire change. To create engaging and impactful stories on screen, writers and filmmakers draw inspiration from various sources such as real-life experiences, social issues, historical events, or simply their imagination. Their role is to weave together a narrative that resonates with audiences while staying true to the essence of the story they want to tell.\n",
      "\n",
      "For instance, in the case of \"The Girl\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
